"use strict";(self.webpackChunkseatunnel_website=self.webpackChunkseatunnel_website||[]).push([[3169],{15680:(e,n,t)=>{t.d(n,{xA:()=>c,yg:()=>g});var a=t(96540);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=a.createContext({}),u=function(e){var n=a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},c=function(e){var n=u(e.components);return a.createElement(l.Provider,{value:n},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},h=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=u(t),h=r,g=p["".concat(l,".").concat(h)]||p[h]||d[h]||o;return t?a.createElement(g,i(i({ref:n},c),{},{components:t})):a.createElement(g,i({ref:n},c))}));function g(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=h;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[p]="string"==typeof e?e:r,i[1]=s;for(var u=2;u<o;u++)i[u]=t[u];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}h.displayName="MDXCreateElement"},54725:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>u});var a=t(58168),r=(t(96540),t(15680));const o={sidebar_position:6},i="Deploy SeaTunnel Engine In Separated Cluster Mode",s={unversionedId:"seatunnel-engine/separated-cluster-deployment",id:"version-2.3.9/seatunnel-engine/separated-cluster-deployment",title:"Deploy SeaTunnel Engine In Separated Cluster Mode",description:"The Master service and Worker service of SeaTunnel Engine are separated, and each service is a separate process. The Master node is only responsible for job scheduling, RESTful API, task submission, etc., and the Imap data is only stored on the Master node. The Worker node is only responsible for the execution of tasks and does not participate in the election to become the master nor stores Imap data.",source:"@site/versioned_docs/version-2.3.9/seatunnel-engine/separated-cluster-deployment.md",sourceDirName:"seatunnel-engine",slug:"/seatunnel-engine/separated-cluster-deployment",permalink:"/docs/2.3.9/seatunnel-engine/separated-cluster-deployment",draft:!1,editUrl:"https://github.com/apache/seatunnel-website/edit/main/versioned_docs/version-2.3.9/seatunnel-engine/separated-cluster-deployment.md",tags:[],version:"2.3.9",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"docs",previous:{title:"Deploy SeaTunnel Engine Hybrid Mode Cluster",permalink:"/docs/2.3.9/seatunnel-engine/hybrid-cluster-deployment"},next:{title:"Savepoint And Restore With Savepoint",permalink:"/docs/2.3.9/seatunnel-engine/savepoint"}},l={},u=[{value:"1. Download",id:"1-download",level:2},{value:"2. Configure SEATUNNEL_HOME",id:"2-configure-seatunnel_home",level:2},{value:"3. Configure JVM Options For Master Nodes",id:"3-configure-jvm-options-for-master-nodes",level:2},{value:"4. Configure SeaTunnel Engine",id:"4-configure-seatunnel-engine",level:2},{value:"4.1 Setting the backup number of data in Imap (this parameter is not effective on the Worker node)",id:"41-setting-the-backup-number-of-data-in-imap-this-parameter-is-not-effective-on-the-worker-node",level:3},{value:"4.2 Slot configuration (this parameter is not effective on the Master node)",id:"42-slot-configuration-this-parameter-is-not-effective-on-the-master-node",level:3},{value:"4.3 Checkpoint Manager (This parameter is invalid on the Worker node)",id:"43-checkpoint-manager-this-parameter-is-invalid-on-the-worker-node",level:3},{value:"4.4 History Job Expiry Configuration",id:"44-history-job-expiry-configuration",level:3},{value:"4.5 Class Loader Cache Mode",id:"45-class-loader-cache-mode",level:3},{value:"4.6 Persistence Configuration of IMap (This parameter is invalid on the Worker node)",id:"46-persistence-configuration-of-imap-this-parameter-is-invalid-on-the-worker-node",level:3},{value:"4.7 Job Scheduling Strategy",id:"47-job-scheduling-strategy",level:3},{value:"4.8 Coordinator Service",id:"48-coordinator-service",level:3},{value:"5. Configuring SeaTunnel Engine Network Services",id:"5-configuring-seatunnel-engine-network-services",level:2},{value:"5.1 cluster-name",id:"51-cluster-name",level:3},{value:"5.2 network",id:"52-network",level:3},{value:"tcp-ip",id:"tcp-ip",level:4},{value:"6. Starting the SeaTunnel Engine Master Node",id:"6-starting-the-seatunnel-engine-master-node",level:2},{value:"7. Starting The SeaTunnel Engine Worker Node",id:"7-starting-the-seatunnel-engine-worker-node",level:2},{value:"8. Submit And Manage Jobs",id:"8-submit-and-manage-jobs",level:2},{value:"8.1 Submit Jobs With The SeaTunnel Engine Client",id:"81-submit-jobs-with-the-seatunnel-engine-client",level:3},{value:"Installing The SeaTunnel Engine Client",id:"installing-the-seatunnel-engine-client",level:4},{value:"Setting the <code>SEATUNNEL_HOME</code> the same as the server",id:"setting-the-seatunnel_home-the-same-as-the-server",level:5},{value:"Configuring The SeaTunnel Engine Client",id:"configuring-the-seatunnel-engine-client",level:5},{value:"Submitting And Managing Jobs",id:"submitting-and-managing-jobs",level:4},{value:"8.2 Submit Jobs With The REST API",id:"82-submit-jobs-with-the-rest-api",level:3}],c={toc:u},p="wrapper";function d(e){let{components:n,...t}=e;return(0,r.yg)(p,(0,a.A)({},c,t,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"deploy-seatunnel-engine-in-separated-cluster-mode"},"Deploy SeaTunnel Engine In Separated Cluster Mode"),(0,r.yg)("p",null,"The Master service and Worker service of SeaTunnel Engine are separated, and each service is a separate process. The Master node is only responsible for job scheduling, RESTful API, task submission, etc., and the Imap data is only stored on the Master node. The Worker node is only responsible for the execution of tasks and does not participate in the election to become the master nor stores Imap data."),(0,r.yg)("p",null,"Among all the Master nodes, only one Master node works at the same time, and the other Master nodes are in the standby state. When the current Master node fails or the heartbeat times out, a new Master Active node will be elected from the other Master nodes."),(0,r.yg)("p",null,"This is the most recommended usage method. In this mode, the load on the Master will be very low, and the Master has more resources for job scheduling, task fault tolerance index monitoring, and providing RESTful API services, etc., and will have higher stability. At the same time, the Worker node does not store Imap data. All Imap data is stored on the Master node. Even if the Worker node has a high load or crashes, it will not cause the Imap data to be redistributed."),(0,r.yg)("h2",{id:"1-download"},"1. Download"),(0,r.yg)("p",null,(0,r.yg)("a",{parentName:"p",href:"/docs/2.3.9/seatunnel-engine/download-seatunnel"},"Download And Make SeaTunnel Installation Package")),(0,r.yg)("h2",{id:"2-configure-seatunnel_home"},"2. Configure SEATUNNEL_HOME"),(0,r.yg)("p",null,"You can configure ",(0,r.yg)("inlineCode",{parentName:"p"},"SEATUNNEL_HOME")," by adding the ",(0,r.yg)("inlineCode",{parentName:"p"},"/etc/profile.d/seatunnel.sh")," file. The content of ",(0,r.yg)("inlineCode",{parentName:"p"},"/etc/profile.d/seatunnel.sh")," is as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"export SEATUNNEL_HOME=${seatunnel install path}\nexport PATH=$PATH:$SEATUNNEL_HOME/bin\n")),(0,r.yg)("h2",{id:"3-configure-jvm-options-for-master-nodes"},"3. Configure JVM Options For Master Nodes"),(0,r.yg)("p",null,"The JVM parameters of the Master node are configured in the ",(0,r.yg)("inlineCode",{parentName:"p"},"$SEATUNNEL_HOME/config/jvm_master_options")," file."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"# JVM Heap\n-Xms2g\n-Xmx2g\n\n# JVM Dump\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/tmp/seatunnel/dump/zeta-server\n\n# Metaspace\n-XX:MaxMetaspaceSize=2g\n\n# G1GC\n-XX:+UseG1GC\n")),(0,r.yg)("p",null,"The JVM parameters of the Worker node are configured in the ",(0,r.yg)("inlineCode",{parentName:"p"},"$SEATUNNEL_HOME/config/jvm_worker_options")," file."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"# JVM Heap\n-Xms2g\n-Xmx2g\n\n# JVM Dump\n-XX:+HeapDumpOnOutOfMemoryError\n-XX:HeapDumpPath=/tmp/seatunnel/dump/zeta-server\n\n# Metaspace\n-XX:MaxMetaspaceSize=2g\n\n# G1GC\n-XX:+UseG1GC\n")),(0,r.yg)("h2",{id:"4-configure-seatunnel-engine"},"4. Configure SeaTunnel Engine"),(0,r.yg)("p",null,"SeaTunnel Engine provides many functions and needs to be configured in ",(0,r.yg)("inlineCode",{parentName:"p"},"seatunnel.yaml"),"."),(0,r.yg)("h3",{id:"41-setting-the-backup-number-of-data-in-imap-this-parameter-is-not-effective-on-the-worker-node"},"4.1 Setting the backup number of data in Imap (this parameter is not effective on the Worker node)"),(0,r.yg)("p",null,"SeaTunnel Engine implements cluster management based on ",(0,r.yg)("a",{parentName:"p",href:"https://docs.hazelcast.com/imdg/4.1/"},"Hazelcast IMDG"),". The status data of the cluster (job running status, resource status) is stored in ",(0,r.yg)("a",{parentName:"p",href:"https://docs.hazelcast.com/imdg/4.1/data-structures/map"},"Hazelcast IMap"),". The data stored in Hazelcast IMap will be distributed and stored on all nodes of the cluster. Hazelcast partitions the data stored in Imap. Each partition can specify the number of backups. Therefore, SeaTunnel Engine can achieve cluster HA without using other services (such as zookeeper)."),(0,r.yg)("p",null,"The ",(0,r.yg)("inlineCode",{parentName:"p"},"backup count")," is a parameter that defines the number of synchronous backups. For example, if it is set to 1, the backup of the partition will be placed on one other member. If it is set to 2, it will be placed on two other members."),(0,r.yg)("p",null,"We recommend that the value of ",(0,r.yg)("inlineCode",{parentName:"p"},"backup-count")," be ",(0,r.yg)("inlineCode",{parentName:"p"},"max(1, min(5, N/2))"),". ",(0,r.yg)("inlineCode",{parentName:"p"},"N")," is the number of cluster nodes."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"seatunnel:\n    engine:\n        backup-count: 1\n        # other configurations\n")),(0,r.yg)("admonition",{type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"Since in the separated cluster mode, the Worker node does not store Imap data, the ",(0,r.yg)("inlineCode",{parentName:"p"},"backup-count")," configuration of the Worker node is not effective. If the Master and Worker processes are started on the same machine, the Master and Worker will share the ",(0,r.yg)("inlineCode",{parentName:"p"},"seatunnel.yaml")," configuration file. At this time, the Worker node service will ignore the ",(0,r.yg)("inlineCode",{parentName:"p"},"backup-count")," configuration.")),(0,r.yg)("h3",{id:"42-slot-configuration-this-parameter-is-not-effective-on-the-master-node"},"4.2 Slot configuration (this parameter is not effective on the Master node)"),(0,r.yg)("p",null,"The number of Slots determines the number of task groups that can be run in parallel on the cluster node. The number of Slots required by a task is formulated as N = 2 + P (parallelism configured by the task). By default, the number of Slots of SeaTunnel Engine is dynamic, that is, there is no limit on the number. We recommend that the number of Slots be set to twice the number of CPU cores of the node."),(0,r.yg)("p",null,"The configuration of dynamic slot number (default) is as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"seatunnel:\n    engine:\n        slot-service:\n            dynamic-slot: true\n        # other configurations\n")),(0,r.yg)("p",null,"The configuration of static slot number is as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"seatunnel:\n    engine:\n        slot-service:\n            dynamic-slot: false\n            slot-num: 20\n")),(0,r.yg)("admonition",{type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"Since in the separated cluster mode, the Master node does not run tasks, so the Master service will not start the Slot service, and the ",(0,r.yg)("inlineCode",{parentName:"p"},"slot-service")," configuration of the Master node is not effective. If the Master and Worker processes are started on the same machine, the Master and Worker will share the ",(0,r.yg)("inlineCode",{parentName:"p"},"seatunnel.yaml")," configuration file. At this time, the Master node service will ignore the ",(0,r.yg)("inlineCode",{parentName:"p"},"slot-service")," configuration.")),(0,r.yg)("h3",{id:"43-checkpoint-manager-this-parameter-is-invalid-on-the-worker-node"},"4.3 Checkpoint Manager (This parameter is invalid on the Worker node)"),(0,r.yg)("p",null,"Just like Flink, the SeaTunnel Engine supports the Chandy\u2013Lamport algorithm. Therefore, data synchronization without data loss and duplication can be achieved."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"interval")),(0,r.yg)("p",null,"The interval between two checkpoints, in milliseconds. If the ",(0,r.yg)("inlineCode",{parentName:"p"},"checkpoint.interval")," parameter is configured in the ",(0,r.yg)("inlineCode",{parentName:"p"},"env")," of the job configuration file, it will be subject to the setting in the job configuration file."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"timeout")),(0,r.yg)("p",null,"The timeout time of the checkpoint. If the checkpoint cannot be completed within the timeout time, it will trigger a checkpoint failure and the job fails. If the ",(0,r.yg)("inlineCode",{parentName:"p"},"checkpoint.timeout")," parameter is configured in the ",(0,r.yg)("inlineCode",{parentName:"p"},"env")," of the job configuration file, it will be subject to the setting in the job configuration file."),(0,r.yg)("p",null,"Example"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"seatunnel:\n    engine:\n        backup-count: 1\n        print-execution-info-interval: 10\n        slot-service:\n            dynamic-slot: true\n        checkpoint:\n            interval: 300000\n            timeout: 10000\n")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"checkpoint storage")),(0,r.yg)("p",null,"The checkpoint is a fault-tolerant recovery mechanism. This mechanism ensures that when the program is running, even if it suddenly encounters an exception, it can recover by itself. The checkpoints are triggered regularly, and when each checkpoint is performed, each Task will be required to report its own state information (such as which offset has been read when reading Kafka) to the checkpoint thread, which writes it into a distributed storage (or shared storage). When the task fails and then automatically recovers from fault tolerance, or when recovering a previously paused task through the seatunnel.sh -r instruction, the state information of the corresponding job will be loaded from the checkpoint storage, and the job will be recovered based on these state information."),(0,r.yg)("p",null,"If the number of nodes in the cluster is greater than 1, the checkpoint storage must be a distributed storage or a shared storage, so as to ensure that the task state information stored in it can still be loaded on another node after any node fails."),(0,r.yg)("admonition",{type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"The checkpoint configuration is only read by the Master service, and the Worker service will not read the checkpoint configuration. If the Master and Worker processes are started on the same machine, the Master and Worker will share the ",(0,r.yg)("inlineCode",{parentName:"p"},"seatunnel.yaml")," configuration file, and at this time the Worker node service will ignore the ",(0,r.yg)("inlineCode",{parentName:"p"},"checkpoint")," configuration.")),(0,r.yg)("p",null,"For information about checkpoint storage, you can view ",(0,r.yg)("a",{parentName:"p",href:"/docs/2.3.9/seatunnel-engine/checkpoint-storage"},"checkpoint storage"),"."),(0,r.yg)("h3",{id:"44-history-job-expiry-configuration"},"4.4 History Job Expiry Configuration"),(0,r.yg)("p",null,"The information of each completed job, such as status, counters, and error logs, is stored in an IMap object. As the number of running jobs increases, the memory will increase, and eventually the memory will overflow. Therefore, you can adjust the ",(0,r.yg)("inlineCode",{parentName:"p"},"history-job-expire-minutes")," parameter to solve this problem. The time unit of this parameter is minutes. The default value is 1440 minutes, that is, one day."),(0,r.yg)("p",null,"Example"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"seatunnel:\n  engine:\n    history-job-expire-minutes: 1440\n")),(0,r.yg)("h3",{id:"45-class-loader-cache-mode"},"4.5 Class Loader Cache Mode"),(0,r.yg)("p",null,"This configuration mainly solves the problem of resource leakage caused by continuously creating and attempting to destroy class loaders.\nIf you encounter an exception related to metaspace space overflow, you can try to enable this configuration.\nIn order to reduce the frequency of creating class loaders, after enabling this configuration, SeaTunnel will not try to release the corresponding class loader when the job is completed, so that it can be used by subsequent jobs, that is to say, when not too many types of Source/Sink connector are used in the running job, it is more effective.\nThe default value is true.\nExample"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"seatunnel:\n  engine:\n    classloader-cache-mode: true\n")),(0,r.yg)("h3",{id:"46-persistence-configuration-of-imap-this-parameter-is-invalid-on-the-worker-node"},"4.6 Persistence Configuration of IMap (This parameter is invalid on the Worker node)"),(0,r.yg)("admonition",{type:"tip"},(0,r.yg)("p",{parentName:"admonition"},"Since in the separated cluster mode, only the Master node stores IMap data and the Worker node does not store IMap data, the Worker service will not read this parameter item.")),(0,r.yg)("p",null,"In SeaTunnel, we use IMap (a distributed Map that can implement the writing and reading of data across nodes and processes. For detailed information, please refer to ",(0,r.yg)("a",{parentName:"p",href:"https://docs.hazelcast.com/imdg/4.2/data-structures/map"},"hazelcast map"),") to store the state of each task and its task, so that after the node where the task is located fails, the state information of the task before can be obtained on other nodes, thereby recovering the task and realizing the fault tolerance of the task."),(0,r.yg)("p",null,"By default, the information of IMap is only stored in the memory, and we can set the number of replicas of IMap data. For specific reference (4.1 Setting the number of backups of data in IMap), if the number of replicas is 2, it means that each data will be simultaneously stored in 2 different nodes. Once the node fails, the data in IMap will be automatically replenished to the set number of replicas on other nodes. But when all nodes are stopped, the data in IMap will be lost. When the cluster nodes are started again, all previously running tasks will be marked as failed and need to be recovered manually by the user through the seatunnel.sh -r instruction."),(0,r.yg)("p",null,"To solve this problem, we can persist the data in IMap to an external storage such as HDFS, OSS, etc. In this way, even if all nodes are stopped, the data in IMap will not be lost, and when the cluster nodes are started again, all previously running tasks will be automatically recovered."),(0,r.yg)("p",null,"The following describes how to use the MapStore persistence configuration. For detailed information, please refer to ",(0,r.yg)("a",{parentName:"p",href:"https://docs.hazelcast.com/imdg/4.2/data-structures/map"},"hazelcast map")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"type")),(0,r.yg)("p",null,"The type of IMap persistence, currently only supports ",(0,r.yg)("inlineCode",{parentName:"p"},"hdfs"),"."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"namespace")),(0,r.yg)("p",null,"It is used to distinguish the data storage locations of different businesses, such as the OSS bucket name."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"clusterName")),(0,r.yg)("p",null,"This parameter is mainly used for cluster isolation. We can use it to distinguish different clusters, such as cluster1, cluster2, which is also used to distinguish different businesses."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"fs.defaultFS")),(0,r.yg)("p",null,"We use the hdfs api to read and write files, so providing the hdfs configuration is required for using this storage."),(0,r.yg)("p",null,"If you use HDFS, you can configure it like this:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"map:\n  engine*:\n    map-store:\n      enabled: true\n      initial-mode: EAGER\n      factory-class-name: org.apache.seatunnel.engine.server.persistence.FileMapStoreFactory\n      properties:\n        type: hdfs\n        namespace: /tmp/seatunnel/imap\n        clusterName: seatunnel-cluster\n        storage.type: hdfs\n        fs.defaultFS: hdfs://localhost:9000\n")),(0,r.yg)("p",null,"If there is no HDFS and your cluster has only one node, you can configure it like this to use local files:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"map:\n  engine*:\n    map-store:\n      enabled: true\n      initial-mode: EAGER\n      factory-class-name: org.apache.seatunnel.engine.server.persistence.FileMapStoreFactory\n      properties:\n        type: hdfs\n        namespace: /tmp/seatunnel/imap\n        clusterName: seatunnel-cluster\n        storage.type: hdfs\n        fs.defaultFS: file:///\n")),(0,r.yg)("p",null,"If you use OSS, you can configure it like this:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"map:\n  engine*:\n    map-store:\n      enabled: true\n      initial-mode: EAGER\n      factory-class-name: org.apache.seatunnel.engine.server.persistence.FileMapStoreFactory\n      properties:\n        type: hdfs\n        namespace: /tmp/seatunnel/imap\n        clusterName: seatunnel-cluster\n        storage.type: oss\n        block.size: block size(bytes)\n        oss.bucket: oss://bucket name/\n        fs.oss.accessKeyId: OSS access key id\n        fs.oss.accessKeySecret: OSS access key secret\n        fs.oss.endpoint: OSS endpoint\n")),(0,r.yg)("p",null,"Notice: When using OSS, make sure that the following jars are in the lib directory."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"aliyun-sdk-oss-3.13.2.jar\nhadoop-aliyun-3.3.6.jar\njdom2-2.0.6.jar\nnetty-buffer-4.1.89.Final.jar \nnetty-common-4.1.89.Final.jar\nseatunnel-hadoop3-3.1.4-uber.jar\n")),(0,r.yg)("h3",{id:"47-job-scheduling-strategy"},"4.7 Job Scheduling Strategy"),(0,r.yg)("p",null,"When resources are insufficient, the job scheduling strategy can be configured in the following two modes:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"WAIT"),": Wait for resources to be available.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("inlineCode",{parentName:"p"},"REJECT"),": Reject the job, default value."))),(0,r.yg)("p",null,"Example"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"seatunnel:\n  engine:\n    job-schedule-strategy: WAIT\n")),(0,r.yg)("p",null,"When ",(0,r.yg)("inlineCode",{parentName:"p"},"dynamic-slot: true")," is used, the ",(0,r.yg)("inlineCode",{parentName:"p"},"job-schedule-strategy: WAIT")," configuration will become invalid and will be forcibly changed to ",(0,r.yg)("inlineCode",{parentName:"p"},"job-schedule-strategy: REJECT"),", because this parameter is meaningless in dynamic slots."),(0,r.yg)("h3",{id:"48-coordinator-service"},"4.8 Coordinator Service"),(0,r.yg)("p",null,"CoordinatorService responsible for the process of generating each job from a LogicalDag to an ExecutionDag,\nand then to a PhysicalDag. It ultimately creates the JobMaster for the job to handle scheduling, execution, and state monitoring."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"core-thread-num")),(0,r.yg)("p",null,"The corePoolSize of seatunnel coordinator job's executor cached thread pool"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"max-thread-num")),(0,r.yg)("p",null,"The max job count can be executed at same time"),(0,r.yg)("p",null,"Example"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"coordinator-service:\n  core-thread-num: 30\n  max-thread-num: 1000\n")),(0,r.yg)("h2",{id:"5-configuring-seatunnel-engine-network-services"},"5. Configuring SeaTunnel Engine Network Services"),(0,r.yg)("p",null,"All network-related configurations of the SeaTunnel Engine are in the ",(0,r.yg)("inlineCode",{parentName:"p"},"hazelcast-master.yaml")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"hazelcast-worker.yaml")," files."),(0,r.yg)("h3",{id:"51-cluster-name"},"5.1 cluster-name"),(0,r.yg)("p",null,"SeaTunnel Engine nodes use the ",(0,r.yg)("inlineCode",{parentName:"p"},"cluster-name")," to determine whether another node is in the same cluster as themselves. If the cluster names between two nodes are different, the SeaTunnel Engine will reject service requests."),(0,r.yg)("h3",{id:"52-network"},"5.2 network"),(0,r.yg)("p",null,"Based on ",(0,r.yg)("a",{parentName:"p",href:"https://docs.hazelcast.com/imdg/4.1/clusters/discovery-mechanisms"},"Hazelcast"),", a SeaTunnel Engine cluster is a network composed of cluster members running the SeaTunnel Engine server. Cluster members automatically join together to form a cluster. This automatic joining is through the various discovery mechanisms used by cluster members to discover each other."),(0,r.yg)("p",null,"Please note that after the cluster is formed, the communication between cluster members is always through TCP/IP regardless of the discovery mechanism used."),(0,r.yg)("p",null,"The SeaTunnel Engine uses the following discovery mechanisms."),(0,r.yg)("h4",{id:"tcp-ip"},"tcp-ip"),(0,r.yg)("p",null,"You can configure the SeaTunnel Engine as a complete TCP/IP cluster. For configuration details, please refer to the ",(0,r.yg)("a",{parentName:"p",href:"/docs/2.3.9/seatunnel-engine/tcp"},"Discovering Members by TCP section"),"."),(0,r.yg)("p",null,"In the separated cluster mode, the Master and Worker services use different ports."),(0,r.yg)("p",null,"Master node network configuration ",(0,r.yg)("inlineCode",{parentName:"p"},"hazelcast-master.yaml")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"hazelcast:\n  cluster-name: seatunnel\n  network:\n    rest-api:\n      enabled: true\n      endpoint-groups:\n        CLUSTER_WRITE:\n          enabled: true\n        DATA:\n          enabled: true\n    join:\n      tcp-ip:\n        enabled: true\n        member-list:\n          - master-node-1:5801\n          - master-node-2:5801\n          - worker-node-1:5802\n          - worker-node-2:5802\n    port:\n      auto-increment: false\n      port: 5801\n  properties:\n    hazelcast.heartbeat.failuredetector.type: phi-accrual\n    hazelcast.heartbeat.interval.seconds: 2\n    hazelcast.max.no.heartbeat.seconds: 180\n    hazelcast.heartbeat.phiaccrual.failuredetector.threshold: 10\n    hazelcast.heartbeat.phiaccrual.failuredetector.sample.size: 200\n    hazelcast.heartbeat.phiaccrual.failuredetector.min.std.dev.millis: 100\n")),(0,r.yg)("p",null,"Worker node network configuration ",(0,r.yg)("inlineCode",{parentName:"p"},"hazelcast-worker.yaml")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"hazelcast:\n  cluster-name: seatunnel\n  network:\n    join:\n      tcp-ip:\n        enabled: true\n        member-list:\n          - master-node-1:5801\n          - master-node-2:5801\n          - worker-node-1:5802\n          - worker-node-2:5802\n    port:\n      auto-increment: false\n      port: 5802\n  properties:\n    hazelcast.heartbeat.failuredetector.type: phi-accrual\n    hazelcast.heartbeat.interval.seconds: 2\n    hazelcast.max.no.heartbeat.seconds: 180\n    hazelcast.heartbeat.phiaccrual.failuredetector.threshold: 10\n    hazelcast.heartbeat.phiaccrual.failuredetector.sample.size: 200\n    hazelcast.heartbeat.phiaccrual.failuredetector.min.std.dev.millis: 100\n")),(0,r.yg)("p",null,"TCP is the way we recommend to use in a standalone SeaTunnel Engine cluster."),(0,r.yg)("p",null,"On the other hand, Hazelcast provides some other service discovery methods. For details, please refer to ",(0,r.yg)("a",{parentName:"p",href:"https://docs.hazelcast.com/imdg/4.1/clusters/setting-up-clusters"},"hazelcast network"),"."),(0,r.yg)("h2",{id:"6-starting-the-seatunnel-engine-master-node"},"6. Starting the SeaTunnel Engine Master Node"),(0,r.yg)("p",null,"It can be started using the ",(0,r.yg)("inlineCode",{parentName:"p"},"-d")," parameter through the daemon."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"mkdir -p $SEATUNNEL_HOME/logs\n./bin/seatunnel-cluster.sh -d -r master\n")),(0,r.yg)("p",null,"The logs will be written to ",(0,r.yg)("inlineCode",{parentName:"p"},"$SEATUNNEL_HOME/logs/seatunnel-engine-master.log"),"."),(0,r.yg)("h2",{id:"7-starting-the-seatunnel-engine-worker-node"},"7. Starting The SeaTunnel Engine Worker Node"),(0,r.yg)("p",null,"It can be started using the ",(0,r.yg)("inlineCode",{parentName:"p"},"-d")," parameter through the daemon."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-shell"},"mkdir -p $SEATUNNEL_HOME/logs\n./bin/seatunnel-cluster.sh -d -r worker\n")),(0,r.yg)("p",null,"The logs will be written to ",(0,r.yg)("inlineCode",{parentName:"p"},"$SEATUNNEL_HOME/logs/seatunnel-engine-worker.log"),"."),(0,r.yg)("h2",{id:"8-submit-and-manage-jobs"},"8. Submit And Manage Jobs"),(0,r.yg)("h3",{id:"81-submit-jobs-with-the-seatunnel-engine-client"},"8.1 Submit Jobs With The SeaTunnel Engine Client"),(0,r.yg)("h4",{id:"installing-the-seatunnel-engine-client"},"Installing The SeaTunnel Engine Client"),(0,r.yg)("h5",{id:"setting-the-seatunnel_home-the-same-as-the-server"},"Setting the ",(0,r.yg)("inlineCode",{parentName:"h5"},"SEATUNNEL_HOME")," the same as the server"),(0,r.yg)("p",null,"You can configure the ",(0,r.yg)("inlineCode",{parentName:"p"},"SEATUNNEL_HOME")," by adding the ",(0,r.yg)("inlineCode",{parentName:"p"},"/etc/profile.d/seatunnel.sh")," file. The content of ",(0,r.yg)("inlineCode",{parentName:"p"},"/etc/profile.d/seatunnel.sh")," is as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"export SEATUNNEL_HOME=${seatunnel install path}\nexport PATH=$PATH:$SEATUNNEL_HOME/bin\n")),(0,r.yg)("h5",{id:"configuring-the-seatunnel-engine-client"},"Configuring The SeaTunnel Engine Client"),(0,r.yg)("p",null,"All configurations of the SeaTunnel Engine client are in the ",(0,r.yg)("inlineCode",{parentName:"p"},"hazelcast-client.yaml"),"."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"cluster-name")),(0,r.yg)("p",null,"The client must have the same ",(0,r.yg)("inlineCode",{parentName:"p"},"cluster-name")," as the SeaTunnel Engine. Otherwise, the SeaTunnel Engine will reject the client's request."),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"network")),(0,r.yg)("p",null,"All addresses of the SeaTunnel Engine Master nodes need to be added here."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},"hazelcast-client:\n  cluster-name: seatunnel\n  properties:\n    hazelcast.logging.type: log4j2\n  network:\n    cluster-members:\n      - master-node-1:5801\n      - master-node-2:5801\n")),(0,r.yg)("h4",{id:"submitting-and-managing-jobs"},"Submitting And Managing Jobs"),(0,r.yg)("p",null,"Now that the cluster has been deployed, you can complete the job submission and management through the following tutorial: ",(0,r.yg)("a",{parentName:"p",href:"/docs/2.3.9/seatunnel-engine/user-command"},"Submitting And Managing Jobs"),"."),(0,r.yg)("h3",{id:"82-submit-jobs-with-the-rest-api"},"8.2 Submit Jobs With The REST API"),(0,r.yg)("p",null,"The SeaTunnel Engine provides a REST API for submitting and managing jobs. For more information, please refer to ",(0,r.yg)("a",{parentName:"p",href:"/docs/2.3.9/seatunnel-engine/rest-api-v2"},"REST API V2")))}d.isMDXComponent=!0}}]);